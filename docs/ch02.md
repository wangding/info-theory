# 第 2 章 信息的统计度量



$$
\begin{bmatrix}
X \\
P(X)
\end{bmatrix}
 = \begin{Bmatrix}
 x_1 = 0 & x_2 = 1 \\
 p & 1-p
 \end{Bmatrix}
 , 0 \leqslant  p \leqslant 1
$$

$$
\begin{aligned}
H(X) &= -\sum_ip(x_i)logp(x_i) \\
     &= -[plogp+(1-p)log(1-p))] \\
     &= H(p)
\end{aligned}
$$

$$
\begin{aligned}
& a. H(X) \leqslant logn \\
& b. H[\lambda P + (1-\lambda)Q]\geqslant \lambda H(P)+(1-\lambda )H(Q)
\end{aligned}
$$


一、自信息量
	1. 定义
	2. 含义
	3. 联合、条件
	4. 性质
		1) 非负性
		2) 特殊值
		3) 单调减函数
		4) 可加性
		5) 强可加性
复习：
	1. 概率论的基本概念和公式
	2. 对数函数的定义和运算性质

好，咱们开始上课。诸位同学，大家上午好！今天咱们学习第二章信息的统计度量，第一个大问题自信息量。在开始新的内容之前，依照惯例咱们先把上次课的内容复习一下。上次课的内容是第一章绪论，讲了两个大问题：什么是信息以及什么是信息论。
大家需要掌握的问题有这么几个，我提出来，请同学思考、复习一下。
1、现代科学的三大支柱是什么？
2、信息的本体论定义是什么？
3、信息的认识论定义是什么？认识论定义中考虑信息的那三个方面，这三个方面对应的是什么信息？
4、信息论研究的是什么信息？
5、信息有哪些特征？（举出三个就行）
6、信息有哪些性质？（举出三个就行）
7、信息、消息和信号的区别是什么？
8、人类利用信息的历史，有哪几个里程碑？
9、信息论创立的时间、作者以及标志是什么？
	10、描述组成通信系统模型的各个组成部分以及作用，能够画出通信系统模型图（简单和复杂的）。
	11、信息论研究范畴的分类。
好这些问题都是上次课的重点问题，希望大家能够掌握。咱们开始今天的内容。上次课程咱们主要从定性的角度对信息的概念和性质作了研究和讨论。这样并不能满足工程的需要，通信工程需要从定量的角度对信息进行研究。从这次课程开始我们讨论狭义信息论要解决的第一个大问题，即信息如何度量。这个问题是一个最基本的问题，如果这个问题不解决，信道容量的计算都无从谈起。
那么信息到底如何度量呢？我们都知道信息是无形的，因此没有长短、没有体积和重量，这些度量手段是用来度量有形的物质的，在这里都用不上。那么信息作为无形的事物应该如何度量呢？我们在上次课提到信息和能量有很多相似之处，它们都是无形的，我们先看看能量是如何度量，也许能给我们一些启发。能量是无形的，因此也没有长短、体积和重量，但是我们知道能量可以做功，我们用做功的多少，来度量能量的大小。也就是虽然我们不能够直接度量能量，我们可以用能量做功的大小间接来表示能量的大小。因此热量和做功的国际单位都是焦耳，是一个单位。那么信息呢？既然直接度量信息量比较困难，能不能间接度量信息量？能量可以做功。信息能够做什么呢？大家思考一下。上次课我们提到一个经典的论述——没有物质的世界，怎么样？没有能量的世界怎么样？没有信息的世界，怎么样？从这里大家应该知道信息能够干什么了吧。信息的作用是消除不确定性，不确定性就是混乱和没有秩序。获得了信息就可以消除系统的不确定性。因此，可以用系统在获得信息前以及获得信息后，前后不确定性或混乱程度的差异来间接表示获得信息量的大小。是这样的吗？我们通过一个具体案例来分析一下。
假设有一个电路，串连了八盏灯泡，其中有一盏灯是坏的。现在需要找出这盏坏灯。这个问题大家能够解决吗？（电路图如下）






解决的办法是：先以1、2、3、4，四盏灯为一组，测量两端的电阻，假设电阻无限大，则说明坏灯泡在这四个当中；再把1、2作为一组，测量两端的电阻，假设电阻还是无限大，说明坏灯泡在这两个当中，这时只要测电灯泡1两端的电阻就可以了，假设还是无限大的，那么这盏坏灯就是第一盏灯，最终找到了这盏坏灯。这个道理大家明白吗？
如果大家对这个过程没有疑问，我们再进一步分析一下这个检测过程。这个检测坏灯的过程，其实就是一个获取信息的过程。开始检测以前，8盏灯都可能是坏灯，这个时候不确定性最大。经过第一次测试，我们把坏灯的范围缩小到四盏里，这个时候不确定性减小了，我们获得了一部分信息，而获得的这部分信息量的大小就是测量前后系统不确定度减小的大小。同理，第二次检测把坏灯的范围从四盏缩小到两盏，不确定度进一步缩小了，因此又获得了一部分信息量。最后一次测量将坏灯的范围从两盏缩小到一盏，也就是说找到了这盏坏灯，彻底消除了不确定度。因此，我们可以看到，检测前哪一盏灯是坏灯是非常不确定的，通过三次检测最终把不确定消除了，确定了这盏坏灯，检测过程总共获得了三个单位的信息量。这个例子说明：获得信息就是消除不确定度，获得信息量的大小可以用消除不确定度的大小来衡量。大家看出来了吗？
信息量的大小可以用消除系统的不确定性来间接表示，如果这个观念大家可以接受，那么紧接着我们就会面临这样的问题。系统的不确定性如何表示？这个不确定性问题确实有一个非常好的数学描述工具，它就是概率论。概率论和我们学习的很多数学知识的最大区别就是概率论讨论的对象都是不确定的，我们把这种现象称为随机现象。
我们首先快速的复习一下概率论中的一些基本概念和符号表示。这样我们就可以用这个数学工具描述我们要解决的问题了。
概率论是研究随机现象（偶然现象）的规律的科学。
当一定的综合条件实现时，所发生的现象叫做事件。如果在每次试验的结果中，某事件一定发生，则这一事件叫做必然事件；相反，如果某事件一定不发生，则叫做不可能事件。在试验结果中，可能发生、也可能不发生的事件，叫做随机事件（偶然事件）。
用大写字母A，B，C，……表示随机事件，用字母U表示必然事件，V表示不可能事件。
随机事件A在n次试验中发生了m次，则比值m/n叫做随机事件A的频率，记作（frequency）；用公式表示如下：

显然，任何随机事件的频率是介于0与1之间的一个数：

对于必然事件，有
对于不可能事件，有
随机事件在大量重复试验中存在着某种客观规律——频率的稳定性，随机事件发生的可能性可以用一个数来表示，这个介于0与1之间的数叫做随机事件的概率。记作（probability）。
当试验次数n充分大时，随机事件A的频率在它的概率附近摆动，可以认为频率是概率的近似值，即。
显然，任何随机事件的概率是介于0与1之间的一个数：

对于必然事件，有
对于不可能事件，有
因为随机事件的描述不利于数学运算，所以引入了随机变量的概念。随机变量X取某一个取值，认为是一个随机事件。因此随机变量的概率分布可以表示成：

其中称为随机变量X的概率函数，概率函数具有两个性质：
(1) 
(2) 
有了这些概率论的基本概念，我们知道信源作为信息的来源，它当前发出哪个符号确定吗？不确定。因此发出的符号都对应有一个概率值，这个概率值的大小就代表发出这个符号的不确定性，概率值越大不确定性越怎样？越小。反过来，概率越小，不确定性越大。确定性怎样？你们能分析出来吗？用表格表示如下：
概率越小	不确定性越大	确定性越小		信息量越大
概率越大	不确定性越小	确定性越大		信息量越小
概率 = 1	没有不确定性	完全确定		信息量为零
因此简单的信源可以用随机变量来表示，我们把信源的数学模型表示成下面这种形式，可以看出来它就是随机变量的概率分布：

也可以简单记为：

举一个具体的例子：

这个信源总共有几个信源符号？（四个信源符号）假设是发出四种颜色的光，红、绿、黄、蓝，对应着随机变量有四个取值。发出每个颜色的概率不相同，，， 。
既然我们已经给信源建立了数学模型，这个数学模型是什么？（随机变量）距离度量信源每个符号的信息量大小这个问题就已经非常接近了。信源发出一个符号，这个符号携带多少信息量呢？我们知道信息量的大小与发出这个符号的不确定性大小有关，也就是说与这个符号的概率有关。下面我们给出自信息量的定义：

注意：是的函数，而不是的函数，代表信源发出第个符号的不确定性也就是它的概率。可以把它简化记为。
这个公式里涉及到对数函数。这里我们把对数的概念和运算性质复习一下。
称为以为底的对数函数，其中称为真数，称为底数。以10为底的对数称为常用对数，记作。以为底的对数称为自然对数，记作。
对数运算的性质：

对数函数曲线，如下图所示：

对数曲线与X轴的交点坐标是(1，0)，当对数底数大于1时函数是单调递增的。自然对数的曲线始终在斜线的下方。的取值趋于0时，对数的函数值趋于无穷大。
好，我们回过头来接着看自信息量的定义公式。这个公式里面对数的底数是多少？没有写是吧。自信息量的单位取决于对数的底数，我们把不同的底数和单位的关系写成一张表格：
底数	自信息量	单位
2		比特 bit (binary unit)
10		哈特 hart (Hartley)
e		奈特 nat (nature unit)
这三种对数我们都会用到，在作计算题时会用到以2为底和以10为底的对数，因为我们用的计算器没有以2为底对数运算的功能，所以在计算信息量时中间过程要用以10为底的对数计算，然后再把计算结果转换成以2为底的对数形式，这样所有信息量都表示成为比特。在作证明题的时会用到以e为底的对数，因为证明过程中往往要用自然对数的性质，也就是那个自然对数的不等式。
这些单位之间的换算关系大家能推算出来吗？1哈特等于多少比特？咱们一起来算一下。

所以，1 hart = 3.322 bit。这个数值我们在计算题里会经常用到，大家需要准备一个计算器。计算器可以计算以10为底和以e为底的对数，不能计算以2为底的对数，我们求出常用对数后，利用上面的换算关系，转换成比特。举一个例子，比如计算它的信息量：

好，关于自信息量的定义和公式我们就讲这么多。大家需要记住自信息量的公式，而且需要理解自信息量的含义，自信息量的含义是什么？大家说说看。自信息量是信源发出一个符号所携带的信息量，是这个符号不确定度的度量。好，下面咱们作几个例题，来练习一下自信息的公式。
例1：
求信源中每个符号的信息量。
解：

在这里大家需要掌握一下作计算题的书写格式，先写出待求量，再写出计算公式，然后代入数据，最后计算结果，别忘了单位。从这里面帮助大家养成一个严谨、规范化的习惯。
在讲到自信息量的单位时，大家可能会有一个疑问，自信息量的单位比特和计算机里的存储单位比特一样吗？从这个例题大家可以得到答案。是一样的。计算机存储的一位有几个状态？两个，如果这两个状态（或者说符号）是等概率的，你说这个符号的信息量是多少？就是1比特。存储单位的1比特和信息量的1比特是一样的。这也就是为什么我们要求把所有信息量的最后计算结果统一换成以比特为单位的目的和意义所在。
例2：某地二月份天气的概率分布统计如下：

求信源每个符号的信息量。
解：



例3：口袋中有个不同阻值的电阻，设取不同阻值电阻是等概率的，问取阻值的电阻信息量多少？
解：

例4：口袋中放入个不同阻值的电阻，其中的1个，的2个，……, 的个,问取阻值的电阻信息量多少？
解：


如果信源不单单是一个随机变量，再引入一个随机变量。两个随机变量之间有联合和条件的关系，在这种情况下概率就变成了联合概率和条件概率。我们把联合概率和条件概率的概念和用到的公式简单说明一下。有待补充……
联合概率和条件概率计算的信息量分别称为联合自信息量和条件自信息量。


到目前为止，我们已经了解了自信息量的定义和含义，并且通过例题对这些公式进行了练习。下面我们介绍一下自信息函数的性质，自信息函数总共有五个性质，分别是非负性、特殊值、单调递减性、可加性以及强可加性。下面我们分别介绍一下这五个性质。
(1) 非负性 
含义：信源发出符号的概率不论大小，都携带正的信息量，也就是说它都会消除一些不确定性。大家可以思考一下，如果信息量是负的，会怎样？是消除不确定性还是增加不确定性？是增加不确定性。后面我们会碰到这种情况，这是由于干扰造成的。
证明：

好，我们再强调一下这条性质的含义：信源发出符号的概率不论大小，都携带正的信息量，也就是说它可以消除一些不确定性。
(2) 特殊值
含义：信源符号的概率为1时，就没有不确定性了，因此信息量为0。反之，信源符号的概率趋于0时，不确定度趋于无限大，信息量无限大。这两个值可以从函数的图形上看出来。

(3) 单调递减性
含义：信源符号的概率越大，说明这个符号的不确定性越小，信息量就越小。反之，信源符号的概率越小，说明这个符号的不确定性越大，信息量也越大。
单调函数的定义：
对于函数，在中，对任意。有则在内单调增。如果，则在内单调减。
单调函数的判断方法：
如果在区间内连续可导，如果，则在内单调增；如果，则在内单调减。
证明：


(4) 可加性 
含义：联合自信息量、条件自信息量以及自信息量三者之间的关系就是可加性。
证明：

（请学生给出证明过程）

(5) 强可加性 
含义：当随机变量X和随机变量Y相互独立时，联合自信息量、条件自信息量以及自信息量三者之间的关系就是强可加性。
证明：

二、互信息量
	1. 定义
	2. 含义
	3. 联合、条件
	4. 性质
		1) 可以为负
		2) 对称性（或互易性）
		3) 最大值和零值
		4) 可加性

好，咱们开始上课。诸位同学，大家上午好！今天咱们学习第二章信息的统计度量，第二个大问题互信息量。在开始新的内容之前，依照惯例咱们先把上次课的内容复习一下。上次课的内容是第二章信息的统计度量，讲了第一个大问题自信息量。
有这样几个问题需要大家掌握一下。
1、信息量大小是间接测量的，用什么表示信息量的大小？
2、不确定度是用什么数学工具描述的？
3、不确定度和概率的关系是什么？也就是说概率越大，不确定度怎样，信息量怎样？
4、自信息量的公式？自信息量是谁的函数？
5、自信息量的单位是什么？
6、联合自信息量和条件自信息量的公式分别是什么？
7、自信息量的五个性质分别是什么？它们的含义是什么？大家需要会证明。
好这些问题都是上次课的重点问题，希望大家能够掌握。咱们开始今天的内容。上次课咱们学习了自信息量的内容，我们已经能够计算信源发出某个符号所携带信息量的大小了。在通信系统模型中除了要计算信源的信息量，还有一个地方也需要对信息量进行计算，什么地方？老师的职业习惯都喜欢考试，答案是信道。我们还需要计算信道中传输的信息量大小。那么什么是信道？信道就是信息传输的介质，或者更直白一些就是信息传输的通道，作用类似于供水的自来水管道。流经信道的信息量应该有多少呢？我们先凭生活经验来推测一下。首先，我们看一种最理想的信道，没有干扰的信道，流经信道的信息量应该是多少？就是信源发出符号所携带的信息量吧？其次，我们看看有干扰的信道，流经信道的信息量会怎样？这时流经信道的信息量应该没有原来的那么多了吧？这就好像水管中有泄漏的一样。举个更直观的例子，我在这里讲课，假如下面有同学也在说话，你们听到的信息就可能比较模糊或者干脆听不到，是不是获得的信息量少了？这就是有干扰的信道。以上是我们从生活经验得到的结论。那么准确来说应该如何计算信道上的信息量呢？这还是一个信息量度量的问题。具体来说，我们需要解决的问题是，信源发出一个符号，信宿收到一个符号，流经信道的信息量是多少，当然流经信道的信息量同时也是信宿收到的信息量。这就是我们今天要讨论的问题。
流经信道的信息量的度量问题和上次课讲的信息量的度量解决办法是一致的。都是间接度量的办法，信息可以消除系统的不确定性。因此，可以用系统的不确定性或混乱程度来间接表示信息的大小。
我们把刚才提的两种信道，用这个方法再重新考察一下。
(1) 无干扰信道

上面这个图称为信道转移图，左边是信源，右边是信宿，中间代表信道，信源发出两个符号0和1，信宿会收到两个符号也是0和1。从这个例子我们可以看到，当信源发出0符号，信宿会收到0符号，信道上没有干扰。当信宿收到0符号时可以完全确定是信源发出的0符号，因此信宿收到0符号后对信源发出0符号没有不确定性。因为发送和接收是一一对应的。但是信宿在收到0符号之前，对信源发出0符号是有不确定度的，因为它不能确定信源发出0符号还是1符号，当信宿一旦收到0符号后这种不确定性就消除了。这就是我们所说的获取信息就是消除不确定性，消除不确定性的大小就是获得信息量的大小。因此，获得的信息量大小应该等于获得信息前后两个不确定度的差异。
收到0符号前的不确定度是多少？。收到0符号后的不确定度是多少？没有不确定度了。因此获得信息量。没有干扰的信道流经信道的信息量等于信源发出符号的信息量。
(2) 有干扰信道

当然更为一般的情况是信道上有干扰。上面这个信道转移图所描述的是一个有干扰的信道，这种情况在现实中经常会发生。在计算机系统里你可以把0看作是低电平0伏，把1可以看作是高电平3~5伏，由于干扰的作用会把高电平变成低电平。因此，虽然信源发出1符号，但是有0.2的概率信宿收到0符号。这种情况可以理解吗？对于这种情况，信宿收到0符号，对于信源发送的是0符号还是发送的1符号，确定吗？还是不确定。信宿收到0符号，可能是信源发送的0符号，也可能是信源发送的1符号由于干扰变成的0符号。因此，信宿收到0符号前和收到0符号后都存在不确定度，当然前后不确定度的差异就是收到的信息量，这一点也还是成立的。
收到0符号前的不确定度是多少？。收到0符号后的不确定度是多少？收到0符号后的不确定度是以为条件的条件概率所表示的不确定度的信息量，也就是条件自信息量。。那么流经信道的信息量就等于前后不确定度的差。

从这两个例子我们得到一个非常明显的结论，流经信道的信息量大小等于信宿收到信息量的大小，等于信宿收到符号前后不确定的差异。这和上次课测量灯泡的例子得到的结论是一致的。
下面我们给出互信息量的定义：

注意：公式里用的是分号，对数前面没有负号。对数的真数是个分式，分子的条件概率又称为后验概率（这个名称很直观，也就是信宿收到了作为条件或前提，再考察信源发出的概率），分母是先验概率。我们记忆的时候就记住是后验概率除以先验概率再取对数。先验概率这个数学量，在信息论中代表信宿收到符号前对信源发出符号存在的不确定度；后验概率这个数学量，在信息论中代表信宿收到符号后对信源发出符号仍然存在的不确定度。
我们对这个公式进行一下推导：

可以看出：互信息量就是信宿收到符号前后不确定度的差。这和我们刚才举的例子是一致的。因此，我们可以明确互信息量的含义：互信息量是信源发出符号，信宿收到符号，流经信道的信息量，也是信宿收到的信息量，也是收到符号前后对信源发出符号不确定的差异，不确定度的差异大小就是收到信息量的大小。因此互信息量是对信道中信息流量大小进行度量的概念，而自信息量是对信源发出符号的信息量进行度量的概念，但是本质上它们都是对不确定度的度量。
互信息量的单位与信息量单位一致，对数的底数不同单位不同。
如果把信道反过来看，认为信源发出符号，信宿收到符号。这样得到的互信息量为：

如果站在全局的角度来观察信道，互信息量是后验概率除以先验概率，后验概率认为信源发出符号和信宿收到符号之间是有关联关系的，因此联合概率；先验概率认为信源发出符号和信宿收到符号之间是没有关联关系的，因此联合概率。这样互信息量就可以表示为：

这三种表达方式的本质是相同的，都是后验概率除以先验概率，都是前后不确定度的差异。后面我们讨论到互信息量的对称性（或者互易性）时，大家对这个本质的理解会更深刻一下。
好，关于互信息量的定义和公式我们就讲这么多。大家需要记住互信息量的公式，而且需要理解互信息量的含义，互信息量的含义是什么？大家说说看。互信息量是信源发出符号，信宿收到符号，流经信道的信息量，也是信宿收到的信息量，也是收到符号前后对信源发出符号不确定度的差异，不确定度的差异大小就是收到信息量的大小。好，下面咱们做几个例题，来练习一下互信息的公式。
例1：前面课上举的例子，一个串连了八盏灯泡的电路，其中有一盏灯是坏的。找出这盏坏灯，需要测量三次，每一次测量获得多少信息量？
解：
这盏坏灯的先验概率是
第一次测量的后验概率是
获得的信息量是：
第二次测量的后验概率是
获得的信息量是：
第三次测量的后验概率是
获得的信息量是：
这盏灯的自信息量是
从这个例子可以清楚的看到，获取信息量的大小就是消除的不确定度大小，也就是获取信息前后不确定度的差异大小。信息量的大小就是不确定度的大小。
例2：某地二月份天气的概率分布统计如下：

某一天有人告诉你：“今天不是晴天”。把这句话作为收到的消息。计算信源符号关于的信息量。
解：
当收到后，各种天气发生的概率变成了后验概率了。其中，，，。



例3：某人A预先知道他的三位朋友B，C，D中必定会有一人与某晚要到他家来，并且这三人来得可能性均相同，其先验概率为。但是这天上午A接到D的电话，说因故不能来了。我们把上午这次电话作为事件E，那么有后验概率。这天上午，A又接到C的电话，说他因今晚要出席一个重要的会议不能来A家。我们把下午这一次电话作为事件F，那么有后验概率，而。
计算接到上午电话后，A获得关于B，C，D的互信息量。在接到两次电话后，A获得关于B，C，D的互信息量。
解：



如果在互信息量的公式中再引入一个随机变量。两个随机变量之间有联合和条件的关系，在这种情况下就有联合互信息量和条件互信息量的概念。和互信息量的本质没有区别，都是后验概率除以先验概率再取对数。这里分别给出它们的公式，注意公式中的差别。
联合互信息量：

条件互信息量：

这两个公式的区别在于真数的分母部分，要引起注意，不要记混淆了。
到目前为止，我们已经了解了互信息量的定义和含义，并且通过例题对这些公式进行了练习。下面我们介绍互信息函数的性质，来加深对互信息量的理解。互信息函数总共有四个性质，分别是可以为负、对称性、最大值/零值和可加性。下面我们分别介绍一下这四个性质。性质：
(1) 可以为负
含义：自信息量是非负的，说明不论信源发出符号的概率大小，都携带正的信息量。互信息量可以为负，说明流经信道上的信息量，或者说信宿收到的信息量就不一定是正的了，有可能是负的，这是由信道上的干扰造成的。负的信息量会产生什么作用？大家想想看。负的信息量不但不会减小不确定度，反而会增加不确定度。我们在生活中可能会有这样的经验，你不说我还明白，你越说我越糊涂了。这就是因为提供了负的信息量，增加了不确定度。我给大家讲课有没有这样，不讲还明白，越讲越糊涂。
证明：

当时，即时，
当时，即时， 
可见互信息量可以为正，也可以为负。为正的前提是后验概率比先验概率要大，也就是说收到信息后的不确定度要比收到信息前的不确定度小。为负的情况大家也应该明白。
(2) 对称性（或互易性） 
含义：互信息有三个表达形式，你们还有印象吗？这三个表达形式的本质是相同的，都是后验概率与先验概率的比值取对数，反应的都是获取信息前后不确定度的差异，都是信源发出一个符号，信宿收到一个符号，流经信道信息量的大小，也是信宿收到信息量的大小。这三个表达形式的差别，是由于观察角度不同造成的，但是描述的对象都是信道上的信息流量，这一点是相同的，因此这三个表达形式是等价的。
证明：


(3) 最大值和零值
含义：互信息量有一个最大值，这个最大值是多少？是信源发出符号的自信息量大小，互信息量只能比这个值小，不可能比这个值大，最多等于这个值。比这个值小的程度，表示信道中干扰的大小，干扰非常大时，互信息量为零甚至为负值，说明信道上没有传输信息量甚至传输了负的信息量。互信息量取最大值时，信道上没有干扰，也就是说只有在无干扰的信道上传输信息，信道上的信息流量或者信宿收到的信息量才最大，最多也就是信源发出的信息量大小。
证明：（最大值）

观察到互信息量等于自信息量减去一个非负的数，因此要比自信息量本身小。证明的过程则反过来进行。

证明：（零值）

说明如果X、Y随机变量相互独立时，没有信息量流经信道，这时相当于信道断开。也就是说信宿收到符号前后的对与信源发出符号的不确定度没有改变，收到信息之前有多少不确定度，收到信息之后还有这么多不确定度，你说收到了多少信息？当然收到的信息量为零了。就像我曾经举过的例子，我说你们明天上午上课，对于你们明天上午上什么课，这个消息不能起到减小不确定度的作用，也就是说，收到这条消息前后你对于明天上午上什么课的不确定度没有改变，因此你们获得的信息量是零。即使信源发出符号的信息量不为零，但是这些信息量没有到达信宿那里，全都消耗在信道上了，因此这种情况下信道虽有，但是和没有信道一样，因此可以视为信道断开的，相当于X，Y就相互独立了。
(4) 可加性 
含义：联合互信息量、条件互信息量以及互信息量三者之间的关系就是可加性。
证明：

（请学生给出证明过程）

三、熵
	1. 定义
	2. 含义
	3. 联合、条件
	4. 性质
		1) 非负性
		2) 对称性
		3) 确定性
		4) 扩展性
		5) 最大熵定理
		6) 可加性
		7) 强可加性
		8) 极值性
		9) 上凸性
		10) 递增性

诸位同学，大家上午好！咱们开始上课，在开始新的内容之前，按照惯例咱们先把上次课的内容复习一下。上次课的内容是第二章信息的统计度量，讲了一个大问题？是什么？（互信息量）
大家需要掌握的问题有这样几个。
1、流经信道的信息量是如何度量的？
2、互信息量的公式是什么？
3、互信息量的含义是什么？
4、互信息量的三个表达形式是什么？
5、联合互信息量和条件互信息量的公式分别是什么？
6、互信息量的性质分别是什么？分别说明它们的含义。
好，这些问题都是上次课的重点内容，希望大家能够掌握。咱们开始今天的内容。前面课程讲到了自信息量，它是信源发出某个符号所携带的信息量大小，信源各个符号的概率不同，因此每个符号所携带的信息量大小怎么样？也各不相同。因此自信息量也是一个随机变量，随着信源发出符号的不同而变化。现在我们想要找到一个能够对信源信息量进行整体度量的方法，而不是仅考虑单个符号的信息量。也就是说，我们要解决的问题是：从整体上信源有多少不确定度或者多少信息量。这样我们就可以针对不同概率分布的信源，从总体上对它们的信息量进行比较。
请问，这个问题怎么解决？你们有类似的经验吗？举一个例子，假设有两个班，学生数不一定相同，每位学生的信息论期末成绩可能各不相同。要从整体上比较这两个班的考试成绩，应该怎么办？一个学生、一个学生的比显然比不出来，人数还不一样多。怎么比？对，用平均成绩的办法进行比较。
同样道理对信源信息量大小进行整体度量也用平均信息量的概念，也就是熵entropy。只是对于这个平均数的计算，我们需要特别说明一下。大家接触比较多的就像计算平均成绩一样，先把各个分量求总和，然后除以总数，这称为算术平均数。公式：

如果随机变量的n个取值，且都是等概率出现的。算术平均数可以反映整体状况，如果n个取值的概率不同，我们应该把概率考虑进去，这个时候平均数等于每个分量乘以它的概率再求和，称为加权平均数，或者随机变量的数学期望。公式：

你会看到算术平均数其实是加权平均数的特例，当随机变量等概率分布时，算术平均数与加权平均数相等。我们对信源计算信息量的时候就使用加权平均数，一定要考虑符号的概率，我们知道概率小的信息量大，但是由于概率小，它起的作用也小。
下面我们给出熵的定义：

注意：熵就是信源符号的平均信息量。公式里负号是自信息量带的符号，∑是求和运算符号，是n项求和，其中求和的每一项都是概率乘以概率的对数，n项求和的展开式为：。
我们再明确一下熵的含义：熵是信源符号的平均信息量，是信源符号的平均不确定度，是信源概率分布的整体描述。给定信源概率分布后，信源熵是一个固定的常数，因此可以作为信源整体信息量的度量。
熵的单位与自信息量单位一致，对数的底数不同单位不同。
好，关于熵的定义和公式我们就讲这么多。大家需要记住熵的公式，而且需要理解熵的含义，熵的含义是什么？大家说说看。熵是信源符号的平均信息量，是信源符号的平均不确定度，是信源概率分布的整体描述。好，下面咱们做几个例题，来练习一下熵的公式。
例1：某地二月份天气的概率分布统计如下：

计算信源的平均信息量。
解：

例2：电视屏上约有个格点，按每点有10个不同的灰度等级考虑，则共能组成个不同的画面。按等概率计算，平均每个画面可提供的信息量是多少？另外，有一篇千字文章，假定每字可从万字表中任选，则共有篇不同的千字文，仍按照等概率计算，平均每篇千字文可提供的信息量是多少？
解：


请问，这两个信息量那个比较大？画面的信息量比较大。在日常生活中人们经常会说一句话，叫做“一图胜千文”。这种说法在这个例子中从信息论的角度给与了证明，它确实是成立的。
例3：一个布袋内放100个球，其中80个球是红色的，20个是白色的，若随即摸取一个球，猜测其颜色，求平均摸取一次所能获得的自信息量，以及平均信息量。

解：

如果在熵的公式中再引入一个随机变量。两个随机变量之间有联合和条件的关系，在这种情况下就有联合熵和条件熵的概念。它们与熵的本质没有区别，都是平均不确定度的度量，因此联合熵就是联合自信息量的平均值，而条件熵就是条件自信息量的平均值。这里分别给出它们的公式，注意公式中的差别。
联合熵：

条件熵：

注意：
又称为信道疑义度，表示收到后，对随机变量仍然存在不确定度，这是关于的后验不确定度。它又代表信源信息量在信道上的损失大小，因此又称为损失熵。
又称为噪声熵，表示发出随机变量后，对随机变量仍然存在的不确定度。如果信道中没有噪声，发送端和接收端比存在确定的对应关系，发出后必能确定对应的，现在不能完全确定对应的，这显然是由于噪声引起的，因此又称为噪声熵。
针对这个联合和条件熵，我们做一个例题。
例4：已知，XY构成的联合概率为： ，，计算条件熵。（联合熵？）
解：

到目前为止，我们已经了解了熵的定义和含义，并且通过例题对这些公式进行了练习。下面我们介绍熵函数的性质，来加深对熵的理解。熵函数总共有十个性质，分别是非负性、对称性、确定性、扩展性、最大熵定理、可加性、强可加性、极值性、上凸性、递增性。下面我们分别介绍一下这十个性质。性质：
(1) 非负性 
含义：信源每个符号的信息量，是什么？是自信息量，我们知道自信息量是非负的，也就是说不论信源符号概率大小，它的信息量都是正的，那么请问，这些符号信息量的平均值是非负的吗？当然是了。道理上容易理解，大家能够给出数学证明吗？下面我们给出严格的证明。
证明：


(2) 对称性
含义：这个对称性和互信息量的对称性名称相同，但内涵不同，大家需要引起注意，不要搞混淆。熵是描述信源整体不确定度的，交换信源中若干个符号的概率，只影响这些符号的自信息量大小，但是不影响熵的大小，也就是说信源整体的不确定度不变。
例如：


信源Y的概率分布，是对信源X的概率分布中两个符号的概率交换得到的。因此，符号和符号的自信息量有变化，但是两个信源的熵一样。大家能看出来吗？因为，这种交换在求和公式里，只改变求和运算的顺序，对结果没有影响。
证明：无
(3) 确定性 
含义：一个信源不论有多少个符号，只要有一个符号的概率是1，其它符号的概率多少？当然其它符号的概率必然为0，这个信源的熵一定为0。也就是说这个信源是一个确定信源，没有不确定度，自然平均不确定度也就是平均信息量为0。
特别说明一下： 当时，，则
                  当时，，则
大家可以去验证一下。
证明：无
(4) 扩展性 
含义：有n个符号的信源，增加一个概率非常小的符号，当这个新增符号的概率趋于0时，信源熵不变。也就是说，新增的符号对信源不确定度没有任何贡献。
证明：

(5) 最大熵定理 
含义：含有n个符号的信源，虽然包含的符号个数相同，但概率分布不同，则熵不同。当信源符号等概率分布时，熵最大，也就是不确定度最大。请问信源有4个符号，最大熵是多少？信源概率如何分布？8个符号呢？256个符号呢？
证明：

等号成立的条件：信源符号等概率分布

※不等式证明的技巧：
在这里我们提醒大家，信息论课程里有许多不等式的证明。这些不等式的证明大多可以使用自然对数不等式的性质进行证明。证明的基本思路是：一般要求证明的不等式无非是或这两种形式，不论哪种形式都变成这种形式，对于这种形式，把它变成；对于这种形式，把它变成这种形式，因为不等式的不等号方向是小于等于。然后对进行推导合并，在对数函数的真数部分应用性质得出不等式右边对它进行计算得出为零，这样证明就完成了。大家仔细观察一下最大熵定理性质的证明过程，一定要能够掌握这种证明方法。
※二进熵函数及其曲线：
最简单的信源是只有两个符号的信源，信源符号的概率一个是，则另一个是多少？是，信源概率分布如下所示：



称为二进熵函数，函数曲线如图所示。从二进熵函数的曲线可以看出，对于有两个符号的信源来说，只有两个符号的概率相等时，信源总体的不确定度最大，即熵最大。这和日常生活中的经验是一致的，越是等概率或等可能性，不确定度越大。
(6) 可加性 
含义：联合熵、条件熵以及熵，三者之间的关系就是可加性。
证明：

※等式证明的技巧：
信息论课程里还有许多等式的证明，有很大一部分数量的等式，它们的证明过程有非常明显的规律，我们这种规律总结一下。这类等式的证明大多可以使用对数的降阶运算性质进行证明，包括：，或者，甚至。证明的基本思路是：如果要证明的等式是左边一项等于右边的两项或多项的相加或相减，应该从公式的对数真数部分能够找到相乘或相除的运算，利用降阶性质变成对数外面的加减运算。证明可以从右向左证明，也可以从左向右证明，一般证明多个项加减等于一项比较好证明。大家仔细观察一下可加性的证明过程，一定要能够掌握这种证明方法。
（请学生给出证明过程）

(7) 强可加性 
含义：当随机变量X，Y相互独立时，可加性就成为强可加性。
证明：

(8) 极值性 
含义：纯数学。
证明：

(9) 上凸性
含义：熵函数是一个上凸函数。

定义：符合下列表达式的函数为上凸函数：

从平面几何的角度来看，上凸函数就是函数曲线的割线位于函数曲线下方。
证明：

(10) 递增性

含义：含有n个符号的信源，当把第n个符号的概率分成m份，即增加m个信源符号，则信源熵增加了，即信源整体的不确定度增加了。
证明：

※补充：两个不等式关系得证明。

(1) 
证明：


(2) 
证明：

※证明题解题思路：
　　　　　　　　　　　　　　　　　
　　　　等式　（形式）Ａ＝Ｂ＋Ｃ
　　　　　　　　　　　　　　　　　
　　　　　　　　　　　　Ａ≤Ｂ
　　　　不等式（形式）　　　　　　（统一）Ｃ≤０
　　　　　　　　　　　　Ａ≥Ｂ　　　利用

四、平均互信息量
	1. 定义
	2. 含义
	3. 联合、条件
	4. 性质
		1) 非负性
		2) 对称性（或互易性）
		3) 最大值和零值
		4) 凸函数性（略）
		5) 凹函数性（略）
		6) 可加性

诸位同学，大家上午好！咱们开始上课，在开始新的内容之前，按照惯例咱们先把上次课的内容复习一下。上次课的内容是第二章信息的统计度量，讲了一个大问题？是什么？（平均信息量——熵）
大家需要掌握的问题有这样几个。
1、信源发出符号的信息量从整体上如何度量？
2、算术平均数和加权平均数的公式分别是什么？
3、熵的公式是什么？
4、熵的含义是什么？
5、联合熵和条件熵的公式分别是什么？
6、熵的性质分别是什么？分别说明它们的含义。
好，这些问题都是上次课的重点内容，希望大家能够掌握。咱们开始今天的内容。前面课程讲到了自信息量，它是信源发出某个符号所携带的信息量大小，信源各个符号的概率不同，因此每个符号所携带的信息量大小也各不相同。为了从整体上计算信源的信息量，我们引入熵的概念，它就是平均信息量。我们还讲到了，互信息的概念，互信息量是计算当信源发出一个符号，信宿收到一个符号，流经信道的信息量大小。当然，信源发出的符号不同，信宿收到的符号不同，互信息量也各不相同，它也是一个随机变量，无法从整体上描述信道的信息流量。因此，类似于熵的概念，我们引入一个平均互信息量的概念，来对信道中流经的信息量进行整体的度量。解决问题的思路与熵的思路完全相同，用互信息量的加权平均数，来表示整体上互信息量的大小。因此我们直接给出平均互信息量的定义公式，平均互信息量的定义如下：

注意：这个加权平均数乘的是联合概率，因为互信息量中有两个随机变量。平均互信息量等于熵减去条件熵，这个结果可以直接从互信息量的结论推导出来。这里面涉及到数学期望的一个运算性质，两个随机变量相加的数学期望等于两个随机变量先求数学期望再相加。

我们再来明确一下平均互信息量的含义：平均互信息量是信源发出符号，信宿收到符号，流经信道的平均信息量，也是信宿收到符号前后平均不确定度的差异。代表信宿收到符号前对信源的平均不确定度，代表信宿收到符号后对信源仍然存在的平均不确定度，因此也称为信道疑义度，这两个不确定度的差别就是信道中流经的平均信息量大小。同时因为平均互信息量是信源熵减去条件熵，这个条件熵相当于信源信息量由于信道干扰的作用而产生的损失，因此也称为损失熵。给定信道转移概率分布后，平均互信息量是一个固定的常数，因此可以作为信道整体信息量的度量。
平均互信息量的单位与自信息量单位一致，对数的底数不同单位不同。
如果把信道反过来看，认为信源发出符号，信宿收到符号。这样得到的平均互信息量为：

如果站在全局的角度来观察信道，互信息量是后验概率除以先验概率，后验概率认为信源发出符号和信宿收到符号之间是有关联关系的，因此联合概率；先验概率认为信源发出符号和信宿收到符号之间是没有关联关系的，因此联合概率。这样平均互信息量就可以表示为：

这三种表达形式的本质是相同的，都是后验概率除以先验概率，都是前后不确定度的差异。这与我们讨论的互信息量的对称性（或者互易性）时是一致的，只不过这里讨论的是平均值。
好，关于平均互信息量的定义和公式我们就讲这么多。大家需要记住平均互信息量的公式，而且需要理解平均互信息量的含义，平均互信息量的含义是什么？大家说说看。平均互信息量是信源发出符号，信宿收到符号，流经信道的平均信息量，也是信宿收到的平均信息量，也是收到符号前后对信源发出符号平均不确定度的差异，不确定度的差异大小就是收到信息量的大小。好，下面咱们做一个例题，来练习一下平均互信息的公式。
例1：把已知信源接到图示的信道上，求在该信道上传输的平均互信息量、疑义度、噪声熵和联合熵。


解：

如果在平均互信息量的公式中再引入一个随机变量。两个随机变量之间有联合和条件的关系，在这种情况下就有联合平均互信息量和条件平均互信息量的概念。它们与平均互信息量的本质没有区别，都是平均不确定度的度量，因此联合平均互信息量就是联合互信息量的平均值，而条件平均互信息量就是条件互信息量的平均值。这里分别给出它们的公式，注意公式中的差别。
联合平均互信息量：

条件平均互信息量：

到目前为止，我们已经了解了平均互信息的定义和含义，并且通过例题对这些公式进行了练习。下面我们介绍平均互信息的性质，来加深对它的理解。平均互信息总共有六个性质，分别是非负性、对称性、极值性、凸函数性、凹函数性和可加性。下面我们分别介绍一下这六个性质。性质：
(1) 非负性 
含义：我们知道互信息量是可以为负的，这是互信息量的第一条性质。那么，互信息量的平均值可以为负吗？我们说互信息量的平均值非负，也就是说，不论信道中的干扰多大，从整体来说，信道中流经的信息量都是正的，最小为零。
证明：

(2) 对称性（或互易性） 
含义：平均互信息有三个表达形式，这三个表达形式的本质是相同的，都是后验概率与先验概率的比值取对数然后取平均值，反应的都是获取信息前后平均不确定度的差异，都是信源发出符号，信宿收到符号，流经信道平均信息量的大小，也是信宿收到平均信息量的大小。这三个表达形式的差别，是由于观察角度不同造成的，但是观察的内容和对象都是信道上的平均信息流量，这一点是相同的，因此这三个表达形式是等价的。 
证明：

 (3) 最大值和零值 
含义：平均互信息量有一个最大值，这个最大值是多少？是信源发出符号的平均信息量大小，互信息量只能比这个值小，不可能比这个值大，最多等于这个值。比这个值小的程度，表示信道中干扰的大小，干扰非常大时，平均互信息量为零，说明信道上没有传输信息量。平均互信息量取最大值时，信道上没有干扰，也就是说只有在无干扰的信道上传输信息，信道上的平均信息流量或者信宿收到的平均信息量才最大，最多也就是信源熵的大小。
平均互信息量取零值的条件是。说明如果X、Y随机变量相互独立时，没有信息量流经信道，这时相当于信道断开。也就是说信宿收到符号前后的对与信源发出符号的不确定度没有改变，收到信息之前有多少不确定度，收到信息之后还有这么多不确定度，你说收到了多少信息？当然收到的信息量为零了。就像我曾经举过的例子，我说你们明天上午上课，对于你们明天上午上什么课，这个消息不能起到减小不确定度的作用，也就是说，收到这条消息前后你对于明天上午上什么课的不确定度没有改变，因此你们获得的信息量是零。即使信源发出符号的信息量不为零，但是这些信息量没有到达信宿那里，全都消耗在信道上了，因此这种情况下信道虽有，但是和没有信道一样，因此可以视为信道断开的，相当于X，Y就相互独立了。
证明：

（请学生给出证明过程）

(4) 凸函数性
含义：平均互信息函数是信源概率分布的上凸函数。这个凸函数性是信道容量计算的理论依据。如果说平均互信息量既是上凸函数又是下凹函数，这种说法显然是矛盾的，上凸和下凹是针对不同的参量来说的，平均互信息量实际是信源概率分布和信道转移概率这两个参量的函数，上凸和下凹是分别针对这两个参量中的一个而言的，同时另一个被视为常量。

证明：（略，在第四章信道容量中给出证明。）
(5) 凹函数性
含义：平均互信息函数是信道转移概率分布的下凹函数。这个凹函数性是信息率失真函数的理论依据。
定义：符合下列表达式的函数为下凹函数：

证明：（略，在第七章限失真信源编码中给出证明。）
 (6) 可加性 
含义：联合互信息量、条件互信息量以及互信息量，三者之间的关系就是可加性。
证明：

（请学生给出证明过程）

以上就是平均互信息量的全部内容，到此为止我们已经把狭义信息论中关于信息如何度量的问题都介绍到了，总共是四个概念：自信息量、熵、互信息量和平均互信息量。这里面涉及大量的公式，其实最基本的公式就两个，一个是自信息量的公式，另一个是互信息量的公式，大家只要把这两个公式记住。在这两个公式的基础上进行扩展，就引入了其它的概念和公式，自信息量扩展有联合自信息量和条件自信息量，对它们求平均值就是熵、联合熵和条件熵；互信息量扩展有联合互信息量和条件互信息量，对它们求平均值就是平均互信息量、联合平均互信息量和条件平均互信息量，只要搞清楚它们之间的关系，就可以它们门串联起来记忆，一个一个的去记就太吃力了。
各种信息量之间的关系可以借助集合论中的维恩图(John Venn)来帮助记忆。

五、相对熵（连续信源）
	1. 定义
	2. 含义
	3. 条件、联合
	4. 性质
		1) 可以为负
		2) 上凸性和最大值
		3) 可加性
六、平均互信息量（连续信道）
	1. 定义
	2. 性质
七、小结

诸位同学，大家上午好！咱们开始上课，在开始新的内容之前，按照惯例咱们先把上次课的内容复习一下。上次课的内容是第二章信息的统计度量，讲了一个大问题？是什么？（平均互信息量）
大家需要掌握的问题有这样几个。
1、平均互信息量的定义公式是什么？三种表达形式分别是什么？
2、平均互信息量的含义是什么？
3、联合平均互信息和条件平均互信息的公式分别是什么？
4、平均互信息的性质分别是什么？分别说明它们的含义。
5、各种信息量之间的关系，用维恩图说明。
好，这些问题都是上次课的重点内容，希望大家能够掌握。咱们开始今天的内容。
前面课程讲到了信息量的度量，这是香农狭义信息论中的一个最重要也是最基本的问题。我们已经知道了，对于信息大小的度量采用的是间接度量的方法，度量的是系统的不确定度，使用的数学工具是概率论，因此对于信息的度量就是用统计的方法进行度量，这也就是第二章的题目——信息的统计度量所表达的含义。用信源符号的不确定度大小来表示信息的大小，用信宿接收符号前后的不确定度的差异表示信道中流经的信息量，这分别是自信息/熵以及互信息/平均互信息的概念，这四个概念就是第二章信息统计度量的主要内容。
我们在建立这些信息量的概念时，使用的信源模型和信道模型都是最简单的情况，而实际的信源和信道要比这个复杂的多。这种简化的好处在于，我们可以把目光集中在核心问题上，核心问题就是信息如何度量。现在问题得到解决了，我们已经知道信息如何度量了。那就应该回过头来，把信源分一分类，从简单信源到复杂信源，把我们信息度量的概念和方法在不同的信源模型中去应用一下，看看能不能够适用于不同的信源模型，这就是第三章离散信源要说明的内容。也就是说在第二章信息统计度量方法的基础上，把信源变得复杂一些，看看如何度量这些复杂信源的信息量。这样我们先研究一下信源的情况。
信源作为信息的来源，如果信源一次只发出一个信源符号，这种信源称为单符号信源。第二章介绍的信源基本上都是单符号信源。我们先复习一下这种简单的信源。对于单符号信源，如果符号个数是可数或有限的，这种信源称为单符号离散信源；反之，如果信源符号个数是不可数并且是无限的，这种信源称为单符号连续信源。连续信源的内容在本书的第八章，这个内容不作为咱们课上的教学内容，有能力的同学可以课下自己学习。单符号信源不论是离散的还是连续的都用随机变量描述，随机变量的取值就是信源发出的符号，每个符号发出的概率，组成了随机变量的概率空间。单符号离散信源对应的是离散随机变量，单符号连续信源对应的是连续随机变量。单符号离散信源的数学模型如下：

，，
这里有一个问题大家需要搞清楚，代表什么？它代表信源的符号个数，对于单符号离散信源是不是只有一个符号？单符号离散信源的“单符号”意思是一次只发出一个符号，表示这一个符号的选择范围，如果，这个符号的选择范围是；如果，这个符号的选择范围是；如果，这个符号的选择范围是。这个与信源一次发出几个符号没有任何关系。
你可以把交通信号灯看作一个单符号离散信源的例子，这个信源有三个取值，分别是红灯、绿灯和黄灯，出现的概率是相等的。我们对交通信号灯这个信源建立的数学模型如下：

以上就是我们在第二章学习到的最简单的信源，稍为复杂一些的信源可能一次会发出多个信源符号，这种信源称为多符号信源。这种信源大家见过吗？有没有一个感性的认识？多符号信源中最简单的是两个符号的信源，一次发出两个符号，你可以把汽车的尾灯看成是一个多符号信源，汽车尾灯分成左右两边，每一边都有相同颜色的灯组成，因此一次可以发出两个符号。

把汽车尾灯的例子再简化一下，大家可以想象一下，一个灯泡可以看作是一个单符号离散信源吧，它有两个符号，亮和灭（对应于二进制的0和1）。两个灯泡放在一起就成了最简单的多符号信源了，一次同时发出两个符号，分别是亮亮、亮灭、灭亮和灭灭，对应于二进制就是00、01、10和11。我们把这个信源的数学模型描述如下：
一个灯泡：

两个灯泡：

——>
信道一次会收到一个符号序列，这个符号序列由两个符号组成，每个符号相当于一个独立的信源发出的。符号组的每一位对应着一个随机分量，随机分量的个数就是符号组的长度。两个灯泡，符号组的长度是二；三个灯泡，符号组的长度是三，大家可以依此类推，个灯泡，符号组的长度是，信源一次接收个符号。这里需要特别说明一下，随机变量的符号个数，与符号序列的长度没有任何关系，不好搞混淆。两个灯泡，、；三个灯泡，、。
再举一常见的多符号信源的例子。大家在电视上可能见过摇奖的场面，一台简单的摇奖机，从十个号码球中摇出一个数字号码，可以看作一个单符号信源，它有十个符号，从0至9。如果需要摇出七位数的体育彩票。这就可以看成是一个多符号信源，一次同时发出七个符号。

类似于刚才灯泡的例子，我们给这个摇奖机建立一个信源模型。
一位数：

七位数：

体育彩票的多符号信源，、。请问等于多少？也就是说这个摇奖机总共能够摇出多少个7位的彩票号码？个嘛，每个彩票号码是等概率的，概率是，你就知道中奖的几率太小了。我就从来不买，因为我知道我中不了，更形象一点，你就能理解了。你买（1千万）次，保证你能至少中一次。但是你买一次，中的可能性是，不中的可能性是，相当于100%不中。好我们，拉回来。
从上面的两个例子可以看出，多符号信源用随机矢量或随机序列表示，序列的长度代表符号组的位数。表示多符号信源的随机序列相当于多个随机变量的联合。单符号信源可以看作是当多符号信源的随机序列长度时的特例。
好我们对多符号信源的“多符号”已经有了一些感性的认识。多符号信源，往往还要分成平稳和非平稳的、有记忆和无记忆的以及离散和连续的。我们下面就介绍这些概念。
刚才举例讨论的多符号信源还是比较特殊的，很明显符号序列不同位的随机变量具有相同的概率分布，因此这种多符号信源称为多符号平稳信源，就平稳而言还分成宽平稳和严平稳。宽平稳是不同符号位的随机分量概率分布不完全相同，但是概率分布的数字特征（均值、方差等）相同；严平稳是指两个随机变量的概率分布完全相同，因此上面举的两个多符号信源的例子都是严平稳的。如果既不是严平稳，又不是宽平稳，那就是非平稳。多符号不平稳信源，不同符号位的随机变量分布完全不同。现实的信源大多都是这种情况，但是比较复杂，一样都用平稳的数学模型来近似描述。因此我们讨论的多符号离散信源都是严平稳的多符号离散信源。
在刚才摇奖的多符号信源里，摇的是7位的彩票号码，如果换成是11位的手机号码，或者18位的身份证号码，请问和彩票号码的信源有区别吗？每个符号位都可以取0到9十个数字符号，当然肯定符号组的位数不同，这不是本质的区别。还有一个关键的差别，彩票号码的数字位和数字位之间没有关联关系，而手机号码和身份证号码，数字位之间可能有关联关系，否则不能组成合法的手机号或身份证号。以手机号为例，第一位是1，第二位0到9的概率是等概率的1/10，但是在第一位发出的前提下，第二位只能是3。因为手机号都是13××××，所以数字位之间有关联关系。这种关联关系称为记忆性，没有关联关系的称为无记忆信源。更为明显的例子是英语中的字母和字母之间的关联关系。如果第一个字母是q，第二个字母是q和x的概率变为0。
当然随机序列的符号位所对应的随机分量，既可以是离散的也可以是连续的，分别称为多符号离散信源和多符号连续信源。我们只讨论多符号离散信源。
比多符号信源更复杂的信源就是波形信源，时间上是连续的（可以看作是多符号信源中符号序列的位数无限大并且不可数），取值上也是连续的，我们说话的声音就是波形信源，它用随机过程来描述。这种信源咱们不去讨论，但是咱们需要了解一些基本的概念和常识。以声音为例，首先利用采样技术把波形信源变成多符号连续信源，再利用量化技术把多符号连续信源变成多符号离散信源，这样声音信息就完全成为离散信息了，就可以被计算机处理了。这些内容在你们的多媒体课上可能会涉及。道理我们要搞清楚。
通过模/数（A/D）转换器这种设备，给声波拍下许多个“快照”（即进行采样），每一张快照都记录下了原始模拟声波的某一时刻的电压值，将一连串这样的快照连接起来，就形成了声波的振幅。每一秒钟所拍摄的快照数目就称为采样频率，采样频率的单位是Hz（赫兹，即每秒钟多少次）。采样频率越高，就能够“捕捉”到越高频率的声音，但人耳的听觉范围是有限的，只能达到20～20000赫兹, 根据奈圭斯特（Nyquist）定律，如果我们用40000赫兹的采频率进行采样，即可将人耳听觉范围内的所有声音“捕捉”下来。因此标准规定，音频CD的记录精度为16比特，采样频率为44100赫兹，按照这个标准生产出的音频CD所记录的声音，已经基本达到了人类听觉分辨能力的极限。
声音采样后，声音的振幅仍然是连续取值，还需要进一步把它离散化，这就是量化，量化的单位就是比特。该数值决定了数字音频系统将采用多少个“台阶”来表示声波振幅的范围（即动态范围）。每增加一个比特，表示声波振幅的台阶数就要翻一番，同时数据量也要增加一倍。由此可以计算出，一个1比特的数字音频系统可以为我们提供两个台阶。而一个2比特的数字音频系统可以为我们提供四个台阶；3比特提供八个台阶；4比特提供十六个台阶，……依此类推。如果我们继续增加bit数，则量化精度就将以非常快的速度提高（用数学上的话来说，这是一种指数增长关系）。我们可以计算出16比特能够提供65,536个台阶，任何乐队所奏出的音乐都不可能超过这个范围。
把刚才我们讲到的这些信源的概念归纳起来就是下面这张信源分类图：

从这个分类图里，我们可以看到最简单的信源就是单符号离散信源，也就是我们在第二章讨论的大多数信源。这个章节我们把目标锁定在多符号平稳离散信源里，有记忆和无记忆的情况我们会分别进行讨论。当然也是先从简单的入手，先介绍多符号离散无记忆平稳信源，在介绍多符号离散有记忆平稳信源。因为单符号是多符号的特例，因此多符号就不再特别说明了，就称为离散无记忆平稳信源和离散有记忆平稳信源。
我们再对多符号离散信源的数学模型作一个清晰的描述。多符号离散信源用随机矢量或随机序列描述。
符号序列长度时（单符号离散信源）：

，，
符号序列长度时（多符号离散平稳信源）：

，，，
，，
符号序列长度是时（多符号离散平稳信源）：

，，，
，，
符号序列中随机变量的下标各不相同，并不代表它们是不同的随机变量，它们是具有相同概率分布的随机变量，因为是多符号平稳信源。下标代表符号组的第几位，代表位数。代表第一位，代表第二位，代表第位。表示符号组的总共位数，也就是符号组的长度，表示随机变量取值的符号个数，不要混淆。
下面我们先介绍最简单的多符号离散平稳信源——离散无记忆信源。因为各个随机序列的各个分量之间没有关联关系，而且是平稳的，也就是每个分量都具有相同的概率分布，也称为是单符号离散信源的次扩展信源，记为。次扩展的另外一层涵义是说，符号序列长度为的多符号离散信源所发出的符号序列，可以看作是单符号离散信源在个连续时刻发出的符号组合而成。这样对于多符号离散信源，没有必要在同一时刻准备个单符号离散信源，让它们同时发出信号，重复利用一个单符号离散信源次，就可以达到同样的效果。
我们考察一下这种信源的信息量是多少。

可见单符号离散信源的次扩展信源的熵是原熵的倍，多符号离散平稳无记忆信源相当于有个信源同一时间分别独立的发出每一位符号组成一个符号序列，因此符号序列的信息量就是一位信息量的倍，这是比较好理解的。其实上面的结论我们从第二章熵的强可加性可以得出，下面是具体的证明。我们再通过一个例题来验证一下这个结论。

例1：设有一离散无记忆信源，其概率空间为：

求该信源的二次扩展信源的熵。
解：
二次扩展信源输出符号序列及相应概率分布如下：

可以计算二次扩展信源的熵：

原始信源熵为：

可见：

因此在计算无记忆信源的次扩展信源时可以直接利用结论进行计算，这样做的计算量比较小。
多符号离散平稳有记忆信源的信息量又是多少呢？对于记忆特征的描述有两种模型，这两种模型信息量的计算方法不相同。第一种模型认为长的符号组内部是有记忆的，符号组和符号组之间是没有记忆的，显然也是对现实情况的一种简化。第二种模型是当前的符号总和前面个符号有关联关系，这种关联关系像一个滑动窗口，一直延续到无限远，这种模型的信源称为马尔可夫信源，我们在下一次课介绍。
对第一种模型我们根据熵的可加性很容易给出它的信息量描述来。

这个推导思路是先把前个分量看作，按照熵的可加性进行展开；接着再把前个分量再看作，按照熵的可加性进行展开；直到最后一个分量为止。
也可以用直接计算的方法得出相同的结论。

这样计算出来的是整个符号序列的信息量，这个信息量平均到符号组中的每个符号上又有多少信息量呢？这就是平均符号熵的概念。

我们来看一个例题。
例2：设二维离散信源的原始信源的概率分布为：

中前后两个符号的关联关系如下：
  			
			
	7/9	2/9	0
	1/8	3/4	1/8
	0	2/11	9/11
求该多符号信源的熵、原始信源的熵、条件熵以及平均符号熵，并比较它们的大小。
解：
原始信源熵为：

条件熵为：

多符号信源的熵：

平均符号熵：

可以看到，。
当符号序列的长度时，平均符号熵称为极限熵，记为。

实际信源的记忆长度往往非常大，极限熵可以反映出实际信源的信息量。就好像我们看英文文章时，一个字母不但和它前面的字母有关联关系，还它前面的单词也有关联关系，甚至更前面的若干句子都有关联关系，因此这个记忆长度很大。但是实际上时，这个极限熵非常不好计算，因此我们通常都用有限记忆长度的条件熵或平均符号熵来近似看作是极限熵。
下面我们考察几个结论：
(1) 条件熵随着增加而减小。

含义：条件越多信息量越小，记忆长度越长信息量越小。
证明：

 (2) 给定时，平均符号熵大于等于条件熵。

证明：

(3) 平均符号熵随着增加而减小。
证明：
 (4) 极限熵存在，等于时的条件熵。
证明：
