# 第 3 章 离散信源

一、信源模型和分类
	1. 信源的分类
	2. 信源的数学模型
二、离散无记忆信源及其次扩展信源
三、离散有记忆平稳信源
	1. 信息量
	2. 平均符号熵
	3. 极限熵
	4. 几个结论

诸位同学，大家上午好！咱们开始上课，在开始新的内容之前，按照惯例咱们先把上次课的内容复习一下。上次课的内容是第二章信息的统计度量，讲了一个大问题？是什么？（平均互信息量）
大家需要掌握的问题有这样几个。
1、平均互信息量的定义公式是什么？三种表达形式分别是什么？
2、平均互信息量的含义是什么？
3、联合平均互信息和条件平均互信息的公式分别是什么？
4、平均互信息的性质分别是什么？分别说明它们的含义。
5、各种信息量之间的关系，用维恩图说明。
好，这些问题都是上次课的重点内容，希望大家能够掌握。咱们开始今天的内容。
前面课程讲到了信息量的度量，这是香农狭义信息论中的一个最重要也是最基本的问题。我们已经知道了，对于信息大小的度量采用的是间接度量的方法，度量的是系统的不确定度，使用的数学工具是概率论，因此对于信息的度量就是用统计的方法进行度量，这也就是第二章的题目——信息的统计度量所表达的含义。用信源符号的不确定度大小来表示信息的大小，用信宿接收符号前后的不确定度的差异表示信道中流经的信息量，这分别是自信息/熵以及互信息/平均互信息的概念，这四个概念就是第二章信息统计度量的主要内容。
我们在建立这些信息量的概念时，使用的信源模型和信道模型都是最简单的情况，而实际的信源和信道要比这个复杂的多。这种简化的好处在于，我们可以把目光集中在核心问题上，核心问题就是信息如何度量。现在问题得到解决了，我们已经知道信息如何度量了。那就应该回过头来，把信源分一分类，从简单信源到复杂信源，把我们信息度量的概念和方法在不同的信源模型中去应用一下，看看能不能够适用于不同的信源模型，这就是第三章离散信源要说明的内容。也就是说在第二章信息统计度量方法的基础上，把信源变得复杂一些，看看如何度量这些复杂信源的信息量。这样我们先研究一下信源的情况。
信源作为信息的来源，如果信源一次只发出一个信源符号，这种信源称为单符号信源。第二章介绍的信源基本上都是单符号信源。我们先复习一下这种简单的信源。对于单符号信源，如果符号个数是可数或有限的，这种信源称为单符号离散信源；反之，如果信源符号个数是不可数并且是无限的，这种信源称为单符号连续信源。连续信源的内容在本书的第八章，这个内容不作为咱们课上的教学内容，有能力的同学可以课下自己学习。单符号信源不论是离散的还是连续的都用随机变量描述，随机变量的取值就是信源发出的符号，每个符号发出的概率，组成了随机变量的概率空间。单符号离散信源对应的是离散随机变量，单符号连续信源对应的是连续随机变量。单符号离散信源的数学模型如下：

，，
这里有一个问题大家需要搞清楚，代表什么？它代表信源的符号个数，对于单符号离散信源是不是只有一个符号？单符号离散信源的“单符号”意思是一次只发出一个符号，表示这一个符号的选择范围，如果，这个符号的选择范围是；如果，这个符号的选择范围是；如果，这个符号的选择范围是。这个与信源一次发出几个符号没有任何关系。
你可以把交通信号灯看作一个单符号离散信源的例子，这个信源有三个取值，分别是红灯、绿灯和黄灯，出现的概率是相等的。我们对交通信号灯这个信源建立的数学模型如下：

以上就是我们在第二章学习到的最简单的信源，稍为复杂一些的信源可能一次会发出多个信源符号，这种信源称为多符号信源。这种信源大家见过吗？有没有一个感性的认识？多符号信源中最简单的是两个符号的信源，一次发出两个符号，你可以把汽车的尾灯看成是一个多符号信源，汽车尾灯分成左右两边，每一边都有相同颜色的灯组成，因此一次可以发出两个符号。

把汽车尾灯的例子再简化一下，大家可以想象一下，一个灯泡可以看作是一个单符号离散信源吧，它有两个符号，亮和灭（对应于二进制的0和1）。两个灯泡放在一起就成了最简单的多符号信源了，一次同时发出两个符号，分别是亮亮、亮灭、灭亮和灭灭，对应于二进制就是00、01、10和11。我们把这个信源的数学模型描述如下：
一个灯泡：

两个灯泡：

——>
信道一次会收到一个符号序列，这个符号序列由两个符号组成，每个符号相当于一个独立的信源发出的。符号组的每一位对应着一个随机分量，随机分量的个数就是符号组的长度。两个灯泡，符号组的长度是二；三个灯泡，符号组的长度是三，大家可以依此类推，个灯泡，符号组的长度是，信源一次接收个符号。这里需要特别说明一下，随机变量的符号个数，与符号序列的长度没有任何关系，不好搞混淆。两个灯泡，、；三个灯泡，、。
再举一常见的多符号信源的例子。大家在电视上可能见过摇奖的场面，一台简单的摇奖机，从十个号码球中摇出一个数字号码，可以看作一个单符号信源，它有十个符号，从0至9。如果需要摇出七位数的体育彩票。这就可以看成是一个多符号信源，一次同时发出七个符号。

类似于刚才灯泡的例子，我们给这个摇奖机建立一个信源模型。
一位数：

七位数：

体育彩票的多符号信源，、。请问等于多少？也就是说这个摇奖机总共能够摇出多少个7位的彩票号码？个嘛，每个彩票号码是等概率的，概率是，你就知道中奖的几率太小了。我就从来不买，因为我知道我中不了，更形象一点，你就能理解了。你买（1千万）次，保证你能至少中一次。但是你买一次，中的可能性是，不中的可能性是，相当于100%不中。好我们，拉回来。
从上面的两个例子可以看出，多符号信源用随机矢量或随机序列表示，序列的长度代表符号组的位数。表示多符号信源的随机序列相当于多个随机变量的联合。单符号信源可以看作是当多符号信源的随机序列长度时的特例。
好我们对多符号信源的“多符号”已经有了一些感性的认识。多符号信源，往往还要分成平稳和非平稳的、有记忆和无记忆的以及离散和连续的。我们下面就介绍这些概念。
刚才举例讨论的多符号信源还是比较特殊的，很明显符号序列不同位的随机变量具有相同的概率分布，因此这种多符号信源称为多符号平稳信源，就平稳而言还分成宽平稳和严平稳。宽平稳是不同符号位的随机分量概率分布不完全相同，但是概率分布的数字特征（均值、方差等）相同；严平稳是指两个随机变量的概率分布完全相同，因此上面举的两个多符号信源的例子都是严平稳的。如果既不是严平稳，又不是宽平稳，那就是非平稳。多符号不平稳信源，不同符号位的随机变量分布完全不同。现实的信源大多都是这种情况，但是比较复杂，一样都用平稳的数学模型来近似描述。因此我们讨论的多符号离散信源都是严平稳的多符号离散信源。
在刚才摇奖的多符号信源里，摇的是7位的彩票号码，如果换成是11位的手机号码，或者18位的身份证号码，请问和彩票号码的信源有区别吗？每个符号位都可以取0到9十个数字符号，当然肯定符号组的位数不同，这不是本质的区别。还有一个关键的差别，彩票号码的数字位和数字位之间没有关联关系，而手机号码和身份证号码，数字位之间可能有关联关系，否则不能组成合法的手机号或身份证号。以手机号为例，第一位是1，第二位0到9的概率是等概率的1/10，但是在第一位发出的前提下，第二位只能是3。因为手机号都是13××××，所以数字位之间有关联关系。这种关联关系称为记忆性，没有关联关系的称为无记忆信源。更为明显的例子是英语中的字母和字母之间的关联关系。如果第一个字母是q，第二个字母是q和x的概率变为0。
当然随机序列的符号位所对应的随机分量，既可以是离散的也可以是连续的，分别称为多符号离散信源和多符号连续信源。我们只讨论多符号离散信源。
比多符号信源更复杂的信源就是波形信源，时间上是连续的（可以看作是多符号信源中符号序列的位数无限大并且不可数），取值上也是连续的，我们说话的声音就是波形信源，它用随机过程来描述。这种信源咱们不去讨论，但是咱们需要了解一些基本的概念和常识。以声音为例，首先利用采样技术把波形信源变成多符号连续信源，再利用量化技术把多符号连续信源变成多符号离散信源，这样声音信息就完全成为离散信息了，就可以被计算机处理了。这些内容在你们的多媒体课上可能会涉及。道理我们要搞清楚。
通过模/数（A/D）转换器这种设备，给声波拍下许多个“快照”（即进行采样），每一张快照都记录下了原始模拟声波的某一时刻的电压值，将一连串这样的快照连接起来，就形成了声波的振幅。每一秒钟所拍摄的快照数目就称为采样频率，采样频率的单位是Hz（赫兹，即每秒钟多少次）。采样频率越高，就能够“捕捉”到越高频率的声音，但人耳的听觉范围是有限的，只能达到20～20000赫兹, 根据奈圭斯特（Nyquist）定律，如果我们用40000赫兹的采频率进行采样，即可将人耳听觉范围内的所有声音“捕捉”下来。因此标准规定，音频CD的记录精度为16比特，采样频率为44100赫兹，按照这个标准生产出的音频CD所记录的声音，已经基本达到了人类听觉分辨能力的极限。
声音采样后，声音的振幅仍然是连续取值，还需要进一步把它离散化，这就是量化，量化的单位就是比特。该数值决定了数字音频系统将采用多少个“台阶”来表示声波振幅的范围（即动态范围）。每增加一个比特，表示声波振幅的台阶数就要翻一番，同时数据量也要增加一倍。由此可以计算出，一个1比特的数字音频系统可以为我们提供两个台阶。而一个2比特的数字音频系统可以为我们提供四个台阶；3比特提供八个台阶；4比特提供十六个台阶，……依此类推。如果我们继续增加bit数，则量化精度就将以非常快的速度提高（用数学上的话来说，这是一种指数增长关系）。我们可以计算出16比特能够提供65,536个台阶，任何乐队所奏出的音乐都不可能超过这个范围。
把刚才我们讲到的这些信源的概念归纳起来就是下面这张信源分类图：

从这个分类图里，我们可以看到最简单的信源就是单符号离散信源，也就是我们在第二章讨论的大多数信源。这个章节我们把目标锁定在多符号平稳离散信源里，有记忆和无记忆的情况我们会分别进行讨论。当然也是先从简单的入手，先介绍多符号离散无记忆平稳信源，在介绍多符号离散有记忆平稳信源。因为单符号是多符号的特例，因此多符号就不再特别说明了，就称为离散无记忆平稳信源和离散有记忆平稳信源。
我们再对多符号离散信源的数学模型作一个清晰的描述。多符号离散信源用随机矢量或随机序列描述。
符号序列长度时（单符号离散信源）：

，，
符号序列长度时（多符号离散平稳信源）：

，，，
，，
符号序列长度是时（多符号离散平稳信源）：

，，，
，，
符号序列中随机变量的下标各不相同，并不代表它们是不同的随机变量，它们是具有相同概率分布的随机变量，因为是多符号平稳信源。下标代表符号组的第几位，代表位数。代表第一位，代表第二位，代表第位。表示符号组的总共位数，也就是符号组的长度，表示随机变量取值的符号个数，不要混淆。
下面我们先介绍最简单的多符号离散平稳信源——离散无记忆信源。因为各个随机序列的各个分量之间没有关联关系，而且是平稳的，也就是每个分量都具有相同的概率分布，也称为是单符号离散信源的次扩展信源，记为。次扩展的另外一层涵义是说，符号序列长度为的多符号离散信源所发出的符号序列，可以看作是单符号离散信源在个连续时刻发出的符号组合而成。这样对于多符号离散信源，没有必要在同一时刻准备个单符号离散信源，让它们同时发出信号，重复利用一个单符号离散信源次，就可以达到同样的效果。
我们考察一下这种信源的信息量是多少。

可见单符号离散信源的次扩展信源的熵是原熵的倍，多符号离散平稳无记忆信源相当于有个信源同一时间分别独立的发出每一位符号组成一个符号序列，因此符号序列的信息量就是一位信息量的倍，这是比较好理解的。其实上面的结论我们从第二章熵的强可加性可以得出，下面是具体的证明。我们再通过一个例题来验证一下这个结论。

例1：设有一离散无记忆信源，其概率空间为：

求该信源的二次扩展信源的熵。
解：
二次扩展信源输出符号序列及相应概率分布如下：

可以计算二次扩展信源的熵：

原始信源熵为：

可见：

因此在计算无记忆信源的次扩展信源时可以直接利用结论进行计算，这样做的计算量比较小。
多符号离散平稳有记忆信源的信息量又是多少呢？对于记忆特征的描述有两种模型，这两种模型信息量的计算方法不相同。第一种模型认为长的符号组内部是有记忆的，符号组和符号组之间是没有记忆的，显然也是对现实情况的一种简化。第二种模型是当前的符号总和前面个符号有关联关系，这种关联关系像一个滑动窗口，一直延续到无限远，这种模型的信源称为马尔可夫信源，我们在下一次课介绍。
对第一种模型我们根据熵的可加性很容易给出它的信息量描述来。

这个推导思路是先把前个分量看作，按照熵的可加性进行展开；接着再把前个分量再看作，按照熵的可加性进行展开；直到最后一个分量为止。
也可以用直接计算的方法得出相同的结论。

这样计算出来的是整个符号序列的信息量，这个信息量平均到符号组中的每个符号上又有多少信息量呢？这就是平均符号熵的概念。

我们来看一个例题。
例2：设二维离散信源的原始信源的概率分布为：

中前后两个符号的关联关系如下：
  			
			
	7/9	2/9	0
	1/8	3/4	1/8
	0	2/11	9/11
求该多符号信源的熵、原始信源的熵、条件熵以及平均符号熵，并比较它们的大小。
解：
原始信源熵为：

条件熵为：

多符号信源的熵：

平均符号熵：

可以看到，。
当符号序列的长度时，平均符号熵称为极限熵，记为。

实际信源的记忆长度往往非常大，极限熵可以反映出实际信源的信息量。就好像我们看英文文章时，一个字母不但和它前面的字母有关联关系，还它前面的单词也有关联关系，甚至更前面的若干句子都有关联关系，因此这个记忆长度很大。但是实际上时，这个极限熵非常不好计算，因此我们通常都用有限记忆长度的条件熵或平均符号熵来近似看作是极限熵。
下面我们考察几个结论：
(1) 条件熵随着增加而减小。

含义：条件越多信息量越小，记忆长度越长信息量越小。
证明：

 (2) 给定时，平均符号熵大于等于条件熵。

证明：

(3) 平均符号熵随着增加而减小。
证明：
 (4) 极限熵存在，等于时的条件熵。
证明：


四、马尔可夫信源
	1. 什么是markov过程
	2. 遍历的markov过程
	3. 阶马尔可夫信源熵
五、信源的相关性和剩余度
	1. 记忆长度越大，信源信息量越小
	2. 剩余度
	3. 结论

诸位同学，大家上午好！咱们开始上课，在开始新的内容之前，按照惯例咱们先把上次课的内容复习一下。上次课的内容是第三章离散信源，讲了几个大问题？是什么？（信源的数学模型和分类、离散无记忆信源及其次扩展信源、离散有记忆平稳信源）
大家需要掌握的问题有这样几个。
1、信源分几大类，每一大类又有哪些分支？它们的数学模型是什么？
2、离散无记忆信源的次扩展信源熵是多少？
3、平均符号熵的定义是什么？
4、极限熵的定义是什么？
好，这些问题都是上次课的重点内容，希望大家能够掌握。咱们开始今天的内容。上次课程讲到了多符号有记忆平稳信源，对于记忆特征的描述有两种模型，这两种模型信息量的计算方法不相同。第一种模型认为长的符号组内部是有记忆的，符号组和符号组之间是没有记忆的，显然也是对现实情况的一种简化。这种信源的信息量我们在上次课介绍过了。第二种模型是当前的符号总和前面个符号有关联关系，这种关联关系像一个滑动窗口，一直延续到无限远，这种模型的信源称为马尔可夫信源，这次课我们主要介绍这种信源。
到现在为止，我们所利用的仅仅是信源字母表的发生概率。对自然语言的研究说明在语言中常常还有更多的结构。例如，我们在前面已经谈到过英文字母Q常常后随字母U，所以看到字母U的概率与它前面的那个字母有很大的关系。在概率论中用记号表示“在给定你刚看到的字母是Q的条件下接着会看到U的概率”。在更一般的情况下，阶Markov过程的记号

是在刚看到的字母依次分别是的条件下接着会看到的条件概率。注意这时的符号序列是

显然，这是实际中常常发生的情况——前面部分的消息常常对你即将看到的下一个符号的概率有很大的影响。
用计算机科学中的术语来说，无记忆信源使用信源的每一个符号时与其前面的符号无关。而位记忆信源和它前面的个符号有关，这一概念恰好和阶Markov过程相对应。
我们设想系统有很多状态而它总是处在某一种状态中。对于一个一位记忆信源来讲，它有种状态，信源字母表的每一个符号各对应一个状态。“明天的天气将和今天一样”这种标准的天气预测是一位记忆预测器的一个例子。对一个两位记忆源共有种状态，每种状态对应一对信源符号。线性预测器

是一个两位记忆预测器，下一个值根据前面两个符号和来估计。一般地讲，位记忆源的Markov过程有个状态。
作为简单的一位记忆源的例子，假设我们有一个包合，，三个符号的字母表。在之后出现任何一个符号的概率均为1/3，而在之后出现的概率是1/2，出现或的概率都是1/4。最后，在之后出现的概率是1/2，出现或的概率都是1/4。这样就有
，，
，，
，，
对于这样的Markov过程，用状态转移图来表示是很方便的。在上述情况下当然就有三种状态，分别记以，，并用小圆圈表示。每一根有向线是从一个状态转移到另一个状态，其概率则用线旁的数字标明。例如对应于由到的一条有向线，转移概率是1/4。在这一例子中，每个状态有三条线进，有三条线出。
对这样的图我们可以赋予它一个具体的意义。例如设表示“好天气”，表示“雨”，表示“雪”。如果今天是好天气，那么明天的天气是好，是雨还是雪的可能性就相同。但若今天是雨或是雪，那么明天就会有一半的机会和今天一样，而只有四分之一的机会是其余两种天气中的一种。
我们可以把这张图用下面的办法写成矩阵的形式。现在的状态用矩阵左边的字母来表示，而在转移以后的下一个状态则用矩阵上方的字母来表示。矩阵的元素是转移概率。这样就有

在转移矩阵中，每行元素之和必为1，这是因为现在的状态必定要转移到某一个别的状态。
现在假定今天的天气不是某一个确定的状态，而是一个随机事件，其概率分布为。当然。同时也可能有而其余两个均为零，这就表示今天天气是确定的。假如对于这一行矢量右乘以转移矩阵，就可以得到明天天气的概率分布：

为了得到后天天气的概率分布，我们再一次右乘以转移概率矩阵。根据矩阵(和矢量)相乘的结合律，这也可以先取矩阵的自乘得到矩阵的平方，然后再用它来右乘原先的分布。我们就可得到转移矩阵的平方：

也可以写为

我们马上可以看到，转移矩阵平方中的元素较之原先式所表示的矩阵中的元素来，变化要小。假如我们再取这一矩阵的平方，我们就得到预测今后第四天天气的矩阵。对于这一原始的转移矩阵，不难证明，逐次自乘下去将收敛于一个矩阵。
逐次自乘所得的转移矩阵是否仍然保持每行元素之和等于1这一性质呢？为了证明这一点，我们从两个一般的矩阵和的元素求得乘积矩阵的元素

按假设这两个矩阵均满足行元素之和为1这一条件
，
所以

现在再来看极限矩阵，对应的状态a, b, c的概率分布又是怎样的？显然这一分布不应该再随着日期的推移而改变，对刚才这一矩阵，就意味着必须满足

这等价于下面三个方程式：

或

其中只有两个方程是独立的，所以我们再用

这一条件即可求解此方程组。最后可得平稳解
，
这样，不管初始的状态分布如何，对于转移矩阵式，我们最后都可得到相同的平稳分布：

上面所述的Markov过程一般称为遍历的Markov过程——从系统的任一个状态可以到达任一个其它的状态。而且不管初始状态如何，系统将随着时间的推移进入一个极限分布。这并不是说对于前述矩阵天气将随着时间的过去变成一个雨、雪和晴三种天气的混合，而是说随着时间的过去我们对天气的预测将只是这三种天气的概率值。这一长期预测的概率与今天的天气无关。
除了遍历Markov过程以外，还有很多其它类型的Markov过程。例如图中表示的是一种非遍历性的图。而且根据我们最初所取途径的不同我们可能进入单一的最终状态，也可能在最终时一直环绕着状态，和。我们认为这种类型的过程不是实际信息源的好模型，所以以后不再考虑。
即使在过程最终进入一组状态，而且在这组状态中可以从其中任何一个状态到达其它任意一个状态，也并不意味着概率将趋于某一个值。例如，有一个红黑间隔的方形棋盘，棋子可以在棋盘上作上下左右的移动但不能沿对角线移动。假如我们从红色方格开始，那么经过偶数次转移以后将还在红色的方格上，在经过奇数次转移以后就将在黑色方格上。所以现在所处方格的颜色不同，最后所处方格颜色的概率也不同。因而关于全部可能的Markov过程的理论原则上是相当复杂的。但在我们所关心的信息系统中所遇到的Markov过程却都是遍历的，没有上面这些奇特之处。消息中的符号在经过较长的时间以后最终稳定到一个确定的概率，虽然前面已经指出这不是稳定到一个混合状态而是稳定于一个概率分布。我们不难理解什么是“最终状态”，什么是“最终状态组”。同时我们也不难看到，如果“最终状态组”不包含系统可能有的全部状态，那么系统就不是遍历的。因为遍历性要求系统从任何一个状态可以到达任何一个其它的状态。此外我们还必须避免状态的循环。对于我们可能要对它进行编码的信息源来说我们希望它是一个遍历源，当然其它类型的信息源也不是完全不可能的。如果这种情况真的发生了，问题也不是不能解决的。下面我们假定系统是遍历的。

